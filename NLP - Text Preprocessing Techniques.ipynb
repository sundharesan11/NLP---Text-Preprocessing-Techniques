{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d315f06e",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055419c",
   "metadata": {},
   "source": [
    "## Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f02f7",
   "metadata": {},
   "source": [
    "\n",
    "Text preprocessing is the initial step in Natural Language Processing (NLP) pipelines, essential for transforming raw text data into a format suitable for analysis and modeling. This process involves cleaning and standardizing the text to remove noise, irregularities, and inconsistencies that may impede accurate interpretation by NLP algorithms. Text preprocessing encompasses several tasks aimed at enhancing the quality and usability of textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f08465",
   "metadata": {},
   "source": [
    "###### **Core Preprocessing Steps:**\n",
    "\n",
    "1. Noise Removal\n",
    "2. Normalization\n",
    "3. Tokenization\n",
    "4. Stopword Removal\n",
    "5. Stemming and Lemmatization\n",
    "\n",
    "**Additional Preprocessing Techniques:**\n",
    "\n",
    "Text preprocessing techniques go beyond the core steps and can be applied based on specific needs and domains. These additional techniques include:\n",
    "\n",
    "1. Spell Checking and Correction\n",
    "2. Entity Recognition and Replacement\n",
    "3. Handling Rare or Out-of-Vocabulary (OOV) Words\n",
    "4. Handling Emojis in text\n",
    "\n",
    "By combining both core preprocessing steps and additional techniques, practitioners can tailor their preprocessing approach to suit the requirements of the task at hand and achieve optimal results in NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6700bd",
   "metadata": {},
   "source": [
    "### NOISE REMOVAL:\n",
    "\n",
    "Noise in text data refers to irrelevant or distracting elements that do not contribute to the meaning or analysis of the text. Common types of noise in text data include:\n",
    "1. HTML tags\n",
    "2. Special characters\n",
    "3. Numerical Digits\n",
    "4. Whitespace\n",
    "5. URL's and Email addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da6a3b",
   "metadata": {},
   "source": [
    "To remove noise from text data, various techniques can be employed, including:\n",
    "1. Regular Expressions\n",
    "2. String Manipulation\n",
    "3. Specialized libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92825a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: Hello world This is an example text with special characters and numbers 123\n"
     ]
    }
   ],
   "source": [
    "# With Regular Expressions\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and numerical digits\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "    \n",
    "    # Remove URLs and email addresses\n",
    "    clean_text = re.sub(r'\\b(?:https?|ftp|mailto)://\\S+|www\\.\\S+|[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '', clean_text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Example text with noise\n",
    "text_with_noise = \"<p>Hello, world! This is an example text with special characters: $%^& and numbers: 123.</p>\"\n",
    "clean_text = remove_noise(text_with_noise)\n",
    "print(\"Cleaned Text:\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2d8f5",
   "metadata": {},
   "source": [
    "There are various options available for removing noise using specialized libraries. One such option is BeautifulSoup. For example, we can utilize BeautifulSoup to clean HTML tags as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd3b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Extract text content\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text\n",
    "\n",
    "# Example text with HTML tags\n",
    "html_text = \"<p>Hello, <b>world</b>!</p>\"\n",
    "\n",
    "# Remove HTML tags\n",
    "clean_text = remove_html_tags(html_text)\n",
    "print(\"Cleaned Text:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcd141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33bc5ee9",
   "metadata": {},
   "source": [
    "### NORMALIZTION:\n",
    "\n",
    "Normalization in text preprocessing involves standardizing text data to ensure consistency and facilitate analysis. Common normalization techniques include:\n",
    "1. Converting text tot lowercase or uppercase\n",
    "2. Handling Contrations\n",
    "3. Removing Diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c4f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd73b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: do not you think mr smiths car is not beautiful it is dr browns car\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove diacritics\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example text with variations\n",
    "text_with_variations = \"Don't you think Mr. Smith's car isn't beautiful? It's Dr. Brown's car.\"\n",
    "\n",
    "# Normalize text\n",
    "normalized_text = normalize_text(text_with_variations)\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e013a",
   "metadata": {},
   "source": [
    "###### Explanation:\n",
    "Lowercasing: The text is converted to lowercase using the lower() method.\n",
    "\n",
    "Expanding Contractions: The contractions.fix() function from the contractions library is used to expand contractions in the text. Contractions are shortened versions of words or phrases, such as \"don't\" for \"do not\" or \"isn't\" for \"is not\".\n",
    "\n",
    "Removing Diacritics: In this code, the isalnum() and isspace() methods are used in a list comprehension to remove any characters that are not alphanumeric or whitespace characters. This effectively removes diacritics from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5851b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "163772a4",
   "metadata": {},
   "source": [
    "### **Tokenization:**\n",
    "\n",
    "Tokenization is the process of breaking down raw text into smaller, meaningful units called tokens. These tokens can be words, phrases, or symbols, depending on the specific task or application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0cc97",
   "metadata": {},
   "source": [
    "##### Types of Tokenization:\n",
    "\n",
    "1. **Word Tokenization**:\n",
    "    In word tokenization, the text is split into individual words based on whitespace or punctuation.\n",
    "   \n",
    "\n",
    "2. **Sentence Tokenization**:\n",
    "        Sentence tokenization involves splitting the text into individual sentences based on punctuation marks such as periods, exclamation marks, or question marks.\n",
    "   \n",
    "\n",
    "3. **Whitespace Tokenization**:\n",
    "    Whitespace tokenization splits the text into tokens based solely on whitespace characters (spaces, tabs, newlines).\n",
    "    \n",
    "\n",
    "4. **Regular Expression Tokenization**:\n",
    "    Regular expression tokenization uses predefined patterns to split the text into tokens. This allows for more flexibility in tokenizing text based on specific criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6a925",
   "metadata": {},
   "source": [
    "The code below demonstrates how to perform word, sentence tokenization and WhitespaceTokenizer using the NLTK library in Python.\n",
    "\n",
    "Import NLTK Tokenizers: word_tokenize, sent_tokenize and WhitespaceTokenizer. These functions are used for word, sentence and whitespace tokenization, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed62331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'raw', 'text', 'into', 'smaller', ',', 'meaningful', 'units', 'called', 'tokens', '.', 'It', 'is', 'crucial', 'in', 'natural', 'language', 'processing', '.']\n",
      "\n",
      "Sentence Tokenization: ['Tokenization is the process of breaking down raw text into smaller, meaningful units called tokens.', 'It is crucial in natural language processing.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text \n",
    "text = \"Tokenization is the process of breaking down raw text into smaller, meaningful units called tokens. It is crucial in natural language processing.\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenization:\", words)\n",
    "print(\"\\nSentence Tokenization:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb7d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['The', 'quick-brown,', 'fox;jumps.over!the?lazy/dog.']\n",
      "Regular Expression Tokenization excluding Punctuation: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "\n",
    "#Sample text \n",
    "text = \"The quick-brown, fox;jumps.over!the?lazy/dog.\"\n",
    "\n",
    "# Whitespace tokenization\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "\n",
    "# Regular expression tokenization excluding punctuation\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)\n",
    "print(\"Regular Expression Tokenization excluding Punctuation:\", regex_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c1a1b",
   "metadata": {},
   "source": [
    "While whitespace tokenization is straightforward, regular expression tokenization provides more flexibility and control over the tokenization process. Depending on the complexity of the text and the specific requirements of your NLP task, you may choose the appropriate tokenization method accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0c221",
   "metadata": {},
   "source": [
    "### **Stopword Removal:**\n",
    "\n",
    "Stopwords are common words in a language that often occur frequently but typically do not carry significant meaning or contribute much to the understanding of the text. Examples of stopwords in English include \"the\", \"is\", \"and\", \"to\", etc. Stopword removal is the process of eliminating these stopwords from the text to focus on the more meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ac84e",
   "metadata": {},
   "source": [
    "For handling stopwords, the NLTK toolkit provides a convenient way to access a predefined list of stopwords, which typically consists of the most common words in a language. If you're using NLTK for the first time, you'll need to download the stopwords data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318d4373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sundh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76e516",
   "metadata": {},
   "source": [
    "Once the download is complete, you can load the stopwords package from the NLTK corpus and utilize it in your text preprocessing pipeline.\n",
    "\n",
    "Here's how you can print the list of stopwords in the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd66025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Stopwords in English Language:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the English stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "print(\"List of Stopwords in English Language:\")\n",
    "print(english_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e01977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c085e3fe",
   "metadata": {},
   "source": [
    "The code below removes English stopwords from a sample text using NLTK. It tokenizes the text, filters out stopwords, and reconstructs the text without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08889742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Text after Stopword Removal: quick brown fox jumps lazy dog .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Get the English stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords from the tokens\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Join the filtered tokens back into a sentence\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Stopword Removal:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "001b40e0",
   "metadata": {},
   "source": [
    "### **Stemming and Lemmatization in NLP:**\n",
    "\n",
    "Stemming and lemmatization are techniques used in natural language processing (NLP) to normalize words by reducing them to their base or root forms. While both methods aim to achieve similar results, they differ in their approaches and the level of normalization they provide.\n",
    "\n",
    "### Stemming:\n",
    "\n",
    "Stemming is the process of removing prefixes and suffixes from words to obtain their root or base form, known as the stem. Stemming algorithms apply heuristic rules to trim variations of words, which may not always result in valid words. This method is more aggressive than lemmatization and may produce stems that are not actual words.\n",
    "\n",
    "### Lemmatization:\n",
    "\n",
    "Lemmatization, on the other hand, involves analyzing the morphological structure of words to determine their lemma, which is the canonical or dictionary form of the word. Unlike stemming, lemmatization ensures that the resulting words are valid and exist in the language's dictionary. Lemmatization considers factors such as part of speech and context to accurately determine the lemma of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592e59e",
   "metadata": {},
   "source": [
    "###### **Available Algorithms:**\n",
    "\n",
    "- Porter Stemmer: One of the most widely used stemming algorithms, known for its simplicity and efficiency.\n",
    "- Snowball Stemmer: An extension of the Porter Stemmer, offering support for multiple languages.\n",
    "- Lancaster Stemmer: Known for its aggressive stemming approach, often producing shorter stems.\n",
    "- WordNet Lemmatizer: Based on WordNet lexical database, providing accurate lemmatization based on part of speech.\n",
    "- SpaCy Lemmatizer: Part of the SpaCy library, offering robust lemmatization capabilities with support for multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b853cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The beautiful flowers were blooming in the garden.\n",
      "Stemmed Words: ['the', 'beauti', 'flower', 'were', 'bloom', 'in', 'the', 'garden', '.']\n",
      "Lemmatized Words: ['The', 'beautiful', 'flower', 'were', 'blooming', 'in', 'the', 'garden', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"The beautiful flowers were blooming in the garden.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7aa80",
   "metadata": {},
   "source": [
    "In this example, we use NLTK to perform stemming and lemmatization on the sample text. The PorterStemmer is used for stemming, while the WordNetLemmatizer is used for lemmatization. Each word in the text is processed individually to obtain its stemmed or lemmatized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6288cc",
   "metadata": {},
   "source": [
    "### **Spell Checking and Correction in NLP:**\n",
    "\n",
    "Spell checking and correction is a crucial task in natural language processing (NLP) that aims to identify and rectify misspelled words in text data. Accurate spelling is essential for effective communication and understanding in various NLP applications, such as text processing, search engines, and document analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a139e",
   "metadata": {},
   "source": [
    "###### Types of Spell Checking and Correction:\n",
    "\n",
    "1. Dictionary-Based Approach\n",
    "2. Probabilistic Approach\n",
    "3. Rule-Based Approach\n",
    "###### Algorithms and Techniques:\n",
    "\n",
    "- Edit Distance\n",
    "- Phonetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b9c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d988586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The speeling of thise sentince is incorrrect.\n",
      "Corrected Text: The spelling of this sentence is incorrect.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"The speeling of thise sentince is incorrrect.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Correct spelling errors\n",
    "corrected_text = blob.correct()\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47effc",
   "metadata": {},
   "source": [
    "This code uses TextBlob to create a TextBlob object from the sample text. The correct() method of the TextBlob object automatically corrects spelling errors in the text, and the corrected version is stored in the corrected_text variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17de7f0",
   "metadata": {},
   "source": [
    "### **Entity Recognition and Replacement in NLP:**\n",
    "\n",
    "Entity recognition and replacement is a fundamental task in natural language processing (NLP) that involves identifying and replacing named entities in text data with standardized representations. Named entities are specific objects, people, locations, organizations, dates, and other types of entities mentioned in text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83749a",
   "metadata": {},
   "source": [
    "In this example, we use spaCy to perform named entity recognition and replacement on the sample text. The en_core_web_sm model from spaCy is used to process the text and identify named entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ec4c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Apple Inc. was founded by Steve Jobs and Steve Wozniak in Cupertino, California, in 1976.\n",
      "\n",
      "Replaced Text: ORG ORG was founded by PERSON PERSON and PERSON PERSON in GPE , GPE , in DATE .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NER model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text with named entities\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in Cupertino, California, in 1976.\"\n",
    "\n",
    "# Process the text using spaCy NER model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Replace named entities with generic labels\n",
    "replaced_text = ' '.join([ent.ent_type_ if ent.ent_type_ else ent.text for ent in doc])\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\nReplaced Text:\", replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d1cc4",
   "metadata": {},
   "source": [
    "Here the named entities like \"Apple Inc.\", \"Steve Jobs\", \"Steve Wozniak\", \"Cupertino\", \"California\", and \"1976\" are replaced with their respective generic labels such as \"ORG\" (organization), \"PERSON\" (person), \"GPE\" (geopolitical entity), and \"DATE\" (date)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7bb86",
   "metadata": {},
   "source": [
    "###### Below is the list of some generic labels:\n",
    "['PERSON', 'ORG', 'LOC', 'GPE', 'DATE', 'TIME', 'MONEY', 'PERCENT', 'CARDINAL', 'ORDINAL', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'FAC', 'PRODUCT', 'NORP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf5df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc15523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a8aa4d",
   "metadata": {},
   "source": [
    "### **Handling Emojis in Text**\n",
    "\n",
    "With the increasing prevalence of social media platforms, emojis have become an essential part of online communication. However, for certain text analysis tasks, the presence of emojis may be undesirable. In such cases, a practical solution is to remove emojis from the text. Below is a convenient function that accomplishes this task effectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5594d2",
   "metadata": {},
   "source": [
    "The remove_emoji function below utilizes a regular expression pattern to identify and eliminate emojis from the input string. It covers a wide range of emojis, including emoticons, symbols, flags, and more, ensuring comprehensive removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d26c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "festivities are ongoing \n",
      "Amusing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    # Define the emoji pattern\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Remove emojis from the string\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# Test the function\n",
    "print(remove_emoji(\"festivities are ongoing ðŸ”¥ðŸ”¥\")) \n",
    "print(remove_emoji(\"Amusing ðŸ˜‚\"))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9381e68",
   "metadata": {},
   "source": [
    "Additionally, we may explore methods to convert emojis into their corresponding textual repesentations in future discussions. Stay tuned for further insights on this topic.Additionally, we may explore methods to convert emojis into their corresponding textual repesentations in future discussions. Stay tuned for further insights on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa69c0c",
   "metadata": {},
   "source": [
    "### Closing Statement:\n",
    "We have covered a wide array of text preprocessing techniques essential for Natural Language Processing (NLP) tasks. From the core steps like noise removal, normalization, tokenization, stopword removal, stemming, and lemmatization to additional techniques such as spell checking, entity recognition, handling out-of-vocabulary words, and emoji removal, we've explored the tools and methods necessary for refining and preparing text data for analysis and modeling.\n",
    "\n",
    "These preprocessing steps are crucial for enhancing the accuracy and effectiveness of NLP algorithms, enabling them to extract meaningful insights from textual data. As we continue to delve into more fascinating topics in upcoming articles, spanning various domains of artificial intelligence such as machine learning algorithms, neural networks, computer vision, and more, we'll further unravel the intricacies of text preprocessing and its role in unlocking the full potential of AI applications. Stay tuned for more exciting insights and discoveries in the realm of artificial intelligence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6dfe19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
